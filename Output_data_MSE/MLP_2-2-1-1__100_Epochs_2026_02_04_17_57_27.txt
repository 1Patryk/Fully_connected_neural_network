MLP_2-2-1-1__100_Epochs_2026_02_04_17_57_27.txt

MAPE_training(100): 3.22573
MAPE_validation(100): 2.21728

*Max_training_value[0]: 10
*Min_training_value[0]: 1

*Max_training_value[1]: 100
*Min_training_value[1]: 82

*Max_training_value[2]: 101
*Min_training_value[2]: 92


*Max_validation_value[0]: 8
*Min_validation_value[0]: 2

*Max_validation_value[1]: 98
*Min_validation_value[1]: 86

*Max_validation_value[2]: 100
*Min_validation_value[2]: 94


Number of epochs || MAPE_value_training || MAPE_value_validation

[1]		3.01522		1.9291
[2]		3.22587		2.2159
[3]		3.22612		2.21626
[4]		3.22612		2.21626
[5]		3.22612		2.21626
[6]		3.22612		2.21626
[7]		3.22612		2.21626
[8]		3.22612		2.21626
[9]		3.22612		2.21626
[10]		3.22612		2.21627
[11]		3.22612		2.21627
[12]		3.22611		2.21627
[13]		3.22611		2.21627
[14]		3.22611		2.21627
[15]		3.22611		2.21627
[16]		3.22611		2.21628
[17]		3.22611		2.21628
[18]		3.22611		2.21628
[19]		3.22611		2.21629
[20]		3.2261		2.21629
[21]		3.2261		2.21629
[22]		3.2261		2.2163
[23]		3.2261		2.2163
[24]		3.2261		2.2163
[25]		3.2261		2.21631
[26]		3.2261		2.21631
[27]		3.22609		2.21632
[28]		3.22609		2.21632
[29]		3.22609		2.21632
[30]		3.22609		2.21633
[31]		3.22609		2.21634
[32]		3.22609		2.21634
[33]		3.22608		2.21635
[34]		3.22608		2.21635
[35]		3.22608		2.21636
[36]		3.22608		2.21636
[37]		3.22608		2.21637
[38]		3.22607		2.21637
[39]		3.22607		2.21638
[40]		3.22607		2.21639
[41]		3.22606		2.2164
[42]		3.22606		2.2164
[43]		3.22606		2.21641
[44]		3.22606		2.21642
[45]		3.22605		2.21643
[46]		3.22605		2.21644
[47]		3.22605		2.21644
[48]		3.22605		2.21645
[49]		3.22604		2.21646
[50]		3.22603		2.21647
[51]		3.22604		2.21648
[52]		3.22603		2.21649
[53]		3.22603		2.2165
[54]		3.22602		2.21651
[55]		3.22602		2.21652
[56]		3.22601		2.21653
[57]		3.22601		2.21654
[58]		3.22601		2.21655
[59]		3.226		2.21656
[60]		3.226		2.21657
[61]		3.22599		2.21658
[62]		3.22599		2.2166
[63]		3.22598		2.21661
[64]		3.22598		2.21662
[65]		3.22598		2.21663
[66]		3.22597		2.21665
[67]		3.22597		2.21666
[68]		3.22596		2.21668
[69]		3.22595		2.21669
[70]		3.22595		2.2167
[71]		3.22594		2.21672
[72]		3.22594		2.21673
[73]		3.22593		2.21675
[74]		3.22592		2.21676
[75]		3.22592		2.21678
[76]		3.22591		2.21679
[77]		3.22591		2.21681
[78]		3.2259		2.21682
[79]		3.22589		2.21684
[80]		3.22589		2.21686
[81]		3.22588		2.21688
[82]		3.22588		2.21689
[83]		3.22587		2.21691
[84]		3.22586		2.21693
[85]		3.22585		2.21695
[86]		3.22585		2.21697
[87]		3.22584		2.21699
[88]		3.22583		2.21701
[89]		3.22582		2.21703
[90]		3.22582		2.21705
[91]		3.22581		2.21707
[92]		3.2258		2.21709
[93]		3.2258		2.21711
[94]		3.22578		2.21714
[95]		3.22578		2.21716
[96]		3.22577		2.21718
[97]		3.22576		2.2172
[98]		3.22575		2.21723
[99]		3.22574		2.21725
[100]		3.22573		2.21728


Vector_of_weights_training

Pos. || value

[0][0]		0.390158
[0][1]		-0.369193
[0][2]		-0.460235
[0][3]		0.326537
[1][0]		0.0323991
[1][1]		0.456542
[2][0]		-0.0601465


Vector_of_bias_training

Pos. || value

[0][0]		0.390165
[0][1]		-0.369092
[1][0]		-0.459808
[2][0]		0.289722


Vector_of_weights_validation

Pos. || value

[0][0]		0.39016
[0][1]		-0.36917
[0][2]		-0.460237
[0][3]		0.326516
[1][0]		0.0324099
[1][1]		0.456532
[2][0]		-0.0600645


Vector_of_bias_validation

Pos. || value

[0][0]		0.390164
[0][1]		-0.36909
[1][0]		-0.459806
[2][0]		0.289651
